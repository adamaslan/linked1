{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 18:18:24,924 - INFO - Authenticate via: https://www.linkedin.com/oauth/v2/authorization?response_type=code&client_id=your_client_id&redirect_uri=http://localhost:8000/callback&scope=w_member_social\n",
      "2025-04-01 18:18:35,609 - ERROR - Pipeline failed: Generation failed: Parsing error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-04-01 18:18:35,621 - ERROR - Post creation failed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------- Core Data Structures ----------------------\n",
    "@dataclass\n",
    "class LinkedInPost:\n",
    "    text: str\n",
    "    hashtags: List[str]\n",
    "    media_urn: Optional[str] = None\n",
    "    link_preview: Optional[str] = None\n",
    "    engagement_score: float = 0.0\n",
    "    error: bool = False\n",
    "    error_details: str = \"\"\n",
    "\n",
    "# ---------------------- Ollama Integration Layer ----------------------\n",
    "class DeepSeekClient:\n",
    "    \"\"\"Optimized client for DeepSeek-R1 1.5B model\"\"\"\n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.model_name = \"deepseek-r1:1.5b\"\n",
    "        self.timeout = 150\n",
    "        self._verify_model()\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Ensure model is available and loaded\"\"\"\n",
    "        try:\n",
    "            models = requests.get(f\"{self.base_url}/api/tags\", timeout=10).json()\n",
    "            if not any(self.model_name in m[\"name\"] for m in models.get(\"models\", [])):\n",
    "                self._pull_model()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model verification failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Handle model download with progress tracking\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    logger.info(f\"Download: {json.loads(line).get('status', '')}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_post(self, topic: str, key_points: List[str]) -> LinkedInPost:\n",
    "        \"\"\"Generate optimized LinkedIn content with metadata extraction\"\"\"\n",
    "        prompt = self._build_prompt(topic, key_points)\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"max_tokens\": 500\n",
    "                    }\n",
    "                },\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return self._parse_response(response.json())\n",
    "        except Exception as e:\n",
    "            return LinkedInPost(\n",
    "                text=\"\",\n",
    "                hashtags=[],\n",
    "                error=True,\n",
    "                error_details=str(e)\n",
    "            )\n",
    "\n",
    "    def _build_prompt(self, topic: str, points: List[str]) -> str:\n",
    "        \"\"\"Construct professional content generation prompt\"\"\"\n",
    "        return f\"\"\"You are a LinkedIn content expert. Create a post about {topic} that:\n",
    "- Targets tech professionals and executives\n",
    "- Uses an inspirational yet professional tone\n",
    "- Includes 3-5 industry-specific hashtags\n",
    "- Incorporates these key points: {\", \".join(points)}\n",
    "- Limits to 3 short paragraphs maximum\n",
    "- Adds emojis sparingly for visual appeal\n",
    "\n",
    "Output format (JSON):\n",
    "{{\n",
    "    \"content\": \"post text\",\n",
    "    \"hashtags\": [\"#list\", \"#of\", \"#tags\"],\n",
    "    \"engagement_strategies\": [\"list of strategies used\"]\n",
    "}}\"\"\"\n",
    "\n",
    "    def _parse_response(self, data: dict) -> LinkedInPost:\n",
    "        \"\"\"Parse and validate model output\"\"\"\n",
    "        try:\n",
    "            response = json.loads(data[\"response\"])\n",
    "            return LinkedInPost(\n",
    "                text=response[\"content\"],\n",
    "                hashtags=response[\"hashtags\"],\n",
    "                engagement_score=self._calculate_score(response)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return LinkedInPost(\n",
    "                text=\"\",\n",
    "                hashtags=[],\n",
    "                error=True,\n",
    "                error_details=f\"Parsing error: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    def _calculate_score(self, response: dict) -> float:\n",
    "        \"\"\"Calculate quality score based on content features\"\"\"\n",
    "        strategies = len(response[\"engagement_strategies\"])\n",
    "        hashtags = len(response[\"hashtags\"])\n",
    "        content_len = len(response[\"content\"])\n",
    "        return min((strategies * 0.2) + (hashtags * 0.15) + (content_len/500 * 0.65), 1.0)\n",
    "\n",
    "# ---------------------- LinkedIn API Integration ----------------------\n",
    "class LinkedInManager:\n",
    "    \"\"\"Handles full LinkedIn integration lifecycle\"\"\"\n",
    "    def __init__(self, client_id: str, client_secret: str):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.access_token = None\n",
    "        self.token_expiry = 0\n",
    "        self.headers = {\n",
    "            \"X-Restli-Protocol-Version\": \"2.0.0\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    def authenticate(self):\n",
    "        \"\"\"Handle OAuth 2.0 authentication flow\"\"\"\n",
    "        auth_url = f\"https://www.linkedin.com/oauth/v2/authorization?response_type=code&client_id={self.client_id}&redirect_uri=http://localhost:8000/callback&scope=w_member_social\"\n",
    "        logger.info(f\"Authenticate via: {auth_url}\")\n",
    "        # In production: Implement full OAuth flow with callback handler\n",
    "        \n",
    "        # For demo purposes using env token\n",
    "        self.access_token = os.getenv(\"LINKEDIN_TOKEN\")\n",
    "        self.headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n",
    "\n",
    "    def create_post(self, post: LinkedInPost):\n",
    "        \"\"\"Execute UGC Post creation with error handling\"\"\"\n",
    "        if post.error:\n",
    "            raise ValueError(\"Cannot post content with errors\")\n",
    "\n",
    "        post_data = {\n",
    "            \"author\": \"urn:li:person:{os.getenv('LINKEDIN_USER_URN')}\",\n",
    "            \"lifecycleState\": \"PUBLISHED\",\n",
    "            \"specificContent\": {\n",
    "                \"com.linkedin.ugc.ShareContent\": {\n",
    "                    \"shareCommentary\": {\n",
    "                        \"text\": f\"{post.text}\\n\\n{' '.join(post.hashtags)}\"\n",
    "                    },\n",
    "                    \"shareMediaCategory\": \"NONE\"\n",
    "                }\n",
    "            },\n",
    "            \"visibility\": {\n",
    "                \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if post.media_urn:\n",
    "            post_data[\"specificContent\"][\"com.linkedin.ugc.ShareContent\"][\"media\"] = [\n",
    "                {\"status\": \"READY\", \"media\": post.media_urn}\n",
    "            ]\n",
    "            post_data[\"specificContent\"][\"com.linkedin.ugc.ShareContent\"][\"shareMediaCategory\"] = \"IMAGE\"\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.linkedin.com/v2/ugcPosts\",\n",
    "                headers=self.headers,\n",
    "                json=post_data\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.headers.get(\"X-Restli-Id\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Post failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def upload_media(self, file_path: str) -> Optional[str]:\n",
    "        \"\"\"Handle media upload workflow\"\"\"\n",
    "        try:\n",
    "            # Register upload\n",
    "            register_payload = {\n",
    "                \"registerUploadRequest\": {\n",
    "                    \"recipes\": [\"urn:li:digitalmediaRecipe:feedshare-image\"],\n",
    "                    \"owner\": f\"urn:li:person:{os.getenv('LINKEDIN_USER_URN')}\",\n",
    "                    \"serviceRelationships\": [{\n",
    "                        \"relationshipType\": \"OWNER\",\n",
    "                        \"identifier\": \"urn:li:userGeneratedContent\"\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            register_response = requests.post(\n",
    "                \"https://api.linkedin.com/v2/assets?action=registerUpload\",\n",
    "                headers=self.headers,\n",
    "                json=register_payload\n",
    "            )\n",
    "            register_response.raise_for_status()\n",
    "            \n",
    "            upload_url = register_response.json()[\"value\"][\"uploadMechanism\"][\"com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest\"][\"uploadUrl\"]\n",
    "            asset_urn = register_response.json()[\"value\"][\"asset\"]\n",
    "\n",
    "            # Perform actual upload\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                upload_response = requests.put(upload_url, data=f)\n",
    "                upload_response.raise_for_status()\n",
    "\n",
    "            return asset_urn\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Media upload failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# ---------------------- Pipeline Orchestrator ----------------------\n",
    "class ContentPipeline:\n",
    "    \"\"\"End-to-end content generation and posting system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.llm = DeepSeekClient()\n",
    "        self.linkedin = LinkedInManager(\n",
    "            client_id=os.getenv(\"LINKEDIN_CLIENT_ID\"),\n",
    "            client_secret=os.getenv(\"LINKEDIN_CLIENT_SECRET\")\n",
    "        )\n",
    "        self.linkedin.authenticate()\n",
    "\n",
    "    def execute(self, topic: str, key_points: List[str], image_path: Optional[str] = None):\n",
    "        \"\"\"Full pipeline execution flow\"\"\"\n",
    "        try:\n",
    "            # Generate content\n",
    "            post = self.llm.generate_post(topic, key_points)\n",
    "            if post.error:\n",
    "                raise RuntimeError(f\"Generation failed: {post.error_details}\")\n",
    "\n",
    "            # Handle media\n",
    "            if image_path:\n",
    "                media_urn = self.linkedin.upload_media(image_path)\n",
    "                if media_urn:\n",
    "                    post.media_urn = media_urn\n",
    "                else:\n",
    "                    logger.warning(\"Proceeding without media due to upload failure\")\n",
    "\n",
    "            # Post to LinkedIn\n",
    "            post_id = self.linkedin.create_post(post)\n",
    "            if post_id:\n",
    "                logger.info(f\"Successfully posted content with ID: {post_id}\")\n",
    "                self._save_artifacts(post, post_id)\n",
    "                return post_id\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _save_artifacts(self, post: LinkedInPost, post_id: str):\n",
    "        \"\"\"Persist pipeline artifacts for analysis\"\"\"\n",
    "        artifact = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"post_id\": post_id,\n",
    "            \"content\": post.text,\n",
    "            \"metadata\": {\n",
    "                \"hashtags\": post.hashtags,\n",
    "                \"engagement_score\": post.engagement_score,\n",
    "                \"media_used\": bool(post.media_urn)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f\"post_{post_id}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(artifact, f)\n",
    "\n",
    "# ---------------------- Example Usage ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Environment setup\n",
    "    os.environ[\"LINKEDIN_CLIENT_ID\"] = \"your_client_id\"\n",
    "    os.environ[\"LINKEDIN_CLIENT_SECRET\"] = \"your_client_secret\"\n",
    "    os.environ[\"LINKEDIN_USER_URN\"] = \"your_user_urn\"\n",
    "    os.environ[\"LINKEDIN_TOKEN\"] = \"your_access_token\"\n",
    "\n",
    "    # Execute pipeline\n",
    "    pipeline = ContentPipeline()\n",
    "    post_id = pipeline.execute(\n",
    "        topic=\"AI-Powered Content Generation Pipelines\",\n",
    "        key_points=[\n",
    "            \"Hybrid human-AI workflow integration\",\n",
    "            \"Quality control through RAG architecture\",\n",
    "            \"Ethical considerations in automated content\"\n",
    "        ],\n",
    "        image_path=\"ai_pipeline_diagram.png\"  # Optional\n",
    "    )\n",
    "    \n",
    "    if post_id:\n",
    "        logger.info(f\"Successfully created post: {post_id}\")\n",
    "    else:\n",
    "        logger.error(\"Post creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- RAG Components ----------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "from typing import List, Tuple\n",
    "\n",
    "class CSVProcessor:\n",
    "    \"\"\"Handles CSV-based knowledge base for RAG\"\"\"\n",
    "    def __init__(self, csv_path: str, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self._preprocess_data()\n",
    "        self._create_vector_store()\n",
    "        \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Clean and prepare CSV data\"\"\"\n",
    "        self.df['combined_text'] = self.df.apply(\n",
    "            lambda row: f\"Title: {row.get('title', '')}\\nContent: {row.get('content', '')}\",\n",
    "            axis=1\n",
    "        )\n",
    "        # Add any additional preprocessing here\n",
    "        \n",
    "    def _create_vector_store(self):\n",
    "        \"\"\"Generate embeddings and create FAISS index\"\"\"\n",
    "        texts = self.df['combined_text'].tolist()\n",
    "        embeddings = self.encoder.encode(texts, show_progress_bar=True)\n",
    "        self.vector_store = FAISS.from_embeddings(\n",
    "            text_embeddings=list(zip(texts, embeddings)),\n",
    "            embedding=self.encoder\n",
    "        )\n",
    "    \n",
    "    def retrieve_context(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Retrieve relevant context from CSV data\"\"\"\n",
    "        results = self.vector_store.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Enhanced Generation ----------------------\n",
    "class DeepSeekRAGClient(DeepSeekClient):\n",
    "    \"\"\"RAG-enhanced content generator\"\"\"\n",
    "    def __init__(self, csv_processor: CSVProcessor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.csv_processor = csv_processor\n",
    "        \n",
    "    def generate_post(self, topic: str, key_points: List[str]) -> LinkedInPost:\n",
    "        \"\"\"Generate post with RAG context\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        context = self.csv_processor.retrieve_context(topic)\n",
    "        \n",
    "        # Generate with context-enhanced prompt\n",
    "        prompt = self._build_rag_prompt(topic, key_points, context)\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"max_tokens\": 600}\n",
    "                },\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return self._parse_response(response.json())\n",
    "        except Exception as e:\n",
    "            return LinkedInPost(\n",
    "                text=\"\", hashtags=[], error=True, error_details=str(e)\n",
    "    \n",
    "    def _build_rag_prompt(self, topic: str, points: List[str], context: List[str]) -> str:\n",
    "        \"\"\"Create context-aware prompt\"\"\"\n",
    "        context_str = \"\\n\".join([f\"- {c}\" for c in context])\n",
    "        return f\"\"\"Using the following context:\n",
    "{context_str}\n",
    "\n",
    "Create a LinkedIn post about {topic} that:\n",
    "- Synthesizes key insights from the context\n",
    "- Incorporates these points: {\", \".join(points)}\n",
    "- Maintains professional tone with 3-5 hashtags\n",
    "- Includes data-driven insights where possible\n",
    "- Limits to 300-400 characters\n",
    "\n",
    "Format (JSON):\n",
    "{{\n",
    "    \"content\": \"post text\",\n",
    "    \"hashtags\": [\"#list\"],\n",
    "    \"data_points_used\": [\"list of used statistics/references\"]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Integrated Pipeline ----------------------\n",
    "class RAGContentPipeline(ContentPipeline):\n",
    "    \"\"\"End-to-end pipeline with RAG integration\"\"\"\n",
    "    def __init__(self, csv_path: str):\n",
    "        super().__init__()\n",
    "        self.csv_processor = CSVProcessor(csv_path)\n",
    "        self.llm = DeepSeekRAGClient(self.csv_processor)\n",
    "        \n",
    "    def execute_with_rag(self, topic: str, key_points: List[str], image_path: Optional[str] = None):\n",
    "        \"\"\"Execute full RAG-enhanced pipeline\"\"\"\n",
    "        return super().execute(topic, key_points, image_path)\n",
    "\n",
    "# Example Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with your CSV knowledge base\n",
    "    pipeline = RAGContentPipeline(\"industry_reports.csv\")\n",
    "    \n",
    "    # Execute RAG-enhanced post creation\n",
    "    post_id = pipeline.execute_with_rag(\n",
    "        topic=\"AI in Manufacturing\",\n",
    "        key_points=[\n",
    "            \"Predictive maintenance adoption\",\n",
    "            \"Quality control improvements\",\n",
    "            \"Supply chain optimization\"\n",
    "        ],\n",
    "        image_path=\"factory_ai.png\"\n",
    "    )\n",
    "    \n",
    "    if post_id:\n",
    "        logger.info(f\"RAG-enhanced post created: {post_id}\")\n",
    "    else:\n",
    "        logger.error(\"RAG pipeline failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
